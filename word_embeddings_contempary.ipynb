{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_embeddings_contempary.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN6eKUWB5GVYjXz6wqBzB+C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrewpkitchin/Word-Embeddings/blob/master/word_embeddings_contempary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1lZwGGeuGRM",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we will demonstrate how to use the contempary word embedding models supported by the Gensim library. We will calculate the individual cosine of each enitity and moral standing words.\n",
        "\n",
        "We suggest mounting a google drive to output results as csv files of cosines for each model. \n",
        "\n",
        "See https://github.com/RaRe-Technologies/gensim-data for a list of models and documentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T-7O1q2QfN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dependancies\n",
        "\n",
        "from google.colab import drive\n",
        "import csv, time\n",
        "import numpy as np\n",
        "import gensim.downloader as api"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRUnXwTX1XG-",
        "colab_type": "text"
      },
      "source": [
        "We first mount our google drive and navigate to our desired folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9zX1Ub3Qhyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd drive/My\\ Drive/word2vecProject/csvFiles"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO7x7QZH1MGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_of_models = ['glove-twitter-25']\n",
        "\n",
        "#Google News Model\n",
        "  #'word2vec-google-news-300'\n",
        "#Twitter Models\n",
        "  #'glove-twitter-100', 'glove-twitter-200', 'glove-twitter-25', 'glove-twitter-50', \n",
        "#Wikipedia Models\n",
        "  #'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-wiki-gigaword-50',\n",
        "  #'fasttext-wiki-news-subwords-300'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX1ivGzt2WNd",
        "colab_type": "text"
      },
      "source": [
        "Here we create a function which returns the normalized vector representation of a word from a model. Note: word_vector will be defined as follows:\n",
        "\n",
        "word_vector = api.load(model_name)\n",
        "\n",
        "we will define this when we load and run the models.\n",
        "\n",
        "We also create a function to compute the cosine of two vectors as well as a function to compute the average vector of a list of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNqEAq_C0oAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def norm(word):\n",
        "  return word_vector[word]/np.linalg.norm(word_vector[word])\n",
        " \n",
        "def cosine_similarity(vec1,vec2):\n",
        "  return np.dot(vec1, vec2)/(np.linalg.norm(vec1)* np.linalg.norm(vec2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWPJZkkmWV_a",
        "colab_type": "text"
      },
      "source": [
        "Here we define a function taht first cycles through our lists of words and compute the cosine similarity between each pair."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7WTcc_n-4bh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosines_to_csv(csv_name, list1, list2, model_name):\n",
        "  with open(csv_name, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    list2.insert(0,model_name)\n",
        "\n",
        "    # Write the headings to the csv.\n",
        "    writer.writerow(list2)\n",
        "\n",
        "    list2.pop(0)\n",
        "\n",
        "    for word in list1:\n",
        "      listOfCosines = []\n",
        "      listOfCosines.append(word)\n",
        "      \n",
        "      for entity in list2:\n",
        "        try:\n",
        "          listOfCosines.append(cosine_similarity(norm(word),norm(entity)))\n",
        "        except KeyError:\n",
        "          listOfCosines.append('NA')\n",
        "\n",
        "      # Writing the cosine scores to the csv.\n",
        "      writer.writerow(listOfCosines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS9VPaVpNMX_",
        "colab_type": "text"
      },
      "source": [
        "Lists of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THkaQpjelLcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "moral_standing = ['care', 'cares', 'cared', 'caring', 'help', 'helps', 'helping', 'helped', 'donate', 'donates', 'donating', 'donated', 'aid', 'aiding', 'aided',  'empathy', 'empathetic', 'empathizing', 'empathizes', 'sympathy', 'sympathetic', 'sympathizing', 'sympathizes', 'compassion', 'compassionate']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sJ7iWRKlbSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entities = ['husband','wife','father','mother','son','daughter','brother','sister','uncle','aunt','niece','nephew','grandmother','grandfather','acquaintance','ally','associate','colleague','counterpart','fellow','neighbor','patriot','confidant','companion','partner','supporter','member','follower','emigrant','foreigner','intruder','settler','stranger','visitor','vagrant','opposition','rival','opponent','adversary','competitor','invader','occupier','arab','beggar','blacks','crippled','disabled','jew','mexican','unemployed','native','elderly','indian','woman','chinese','pauper','enemy','villain','crook','delinquent','murderer','robber','thief','deserter','traitor','convict','criminal','felon','offender','scoundrel','animal','ape','bird','elephant','chicken','cow','dog','fish','pig','shark','bear','snake','monkey','lion','nature','forest','lake','mountain','ocean','reef','river','tree','sea','beach','island','coast','earth','planet']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgR4oZO7NEJ_",
        "colab_type": "text"
      },
      "source": [
        "Individual appraoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lblApKjSty70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in list_of_models:\n",
        "  word_vector = api.load(i)\n",
        "\n",
        "  cosines_to_csv('cosines_enitities_and_moral_standings_{}.csv'.format(i), moral_standing, entities, i)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}