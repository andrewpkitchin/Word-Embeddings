{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "calculating_cosines_historic_models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrewpkitchin/moral-concern-word-embeddings/blob/master/calculating_cosines_historic_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G6pw7cWj-hy"
      },
      "source": [
        "In this Python 2 notebook, we will demonstrate how to use the historic word embedding models supported by the Histwords library to generate the results used in [link to paper]. We will calculate the cosine between each focal enitity and each moral concern word.\n",
        "\n",
        "To use this notebook you will need to clone the Histwords github repository found here: https://github.com/williamleif/histwords as well as download the \n",
        "All English (1800s-1990s) pretrained models found here: https://nlp.stanford.edu/projects/histwords/.  \n",
        "\n",
        "Since Python 2 is depreciated in google colab you may have to run this elsewhere. As of the date of upload, this link: https://colab.research.google.com/notebook#create=true&language=python2 will create a Python 2 notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBwcM_CpsqFf"
      },
      "source": [
        "# Dependancies\n",
        "\n",
        "import numpy as np, csv\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swaZDElT-Z8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af5fdfdd-f166-467b-8f93-99aba068daae"
      },
      "source": [
        "# For our purposes we cloned the Histwords github repository to a google drive and call the models from there.\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/word2vecProject/histwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/word2vecProject/histwords\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3Y5JN3kkpA-"
      },
      "source": [
        "Here we create a function which returns the normalized vector representation of a word from a model. We also create a function to compute the cosine similarity of each pair of words from two input lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEzl8TLmtdPK"
      },
      "source": [
        "# Normalizing vectors.\n",
        "\n",
        "def norm(word, startOfDec, model_name):\n",
        "  from representations.embedding import Embedding\n",
        "  if __name__ == \"__main__\":\n",
        "    fiction_embeddings = Embedding.load(\"/content/drive/My Drive/word2vecProject/histwords/embeddings/{}/{}\".format(model_name, startOfDec))\n",
        "\n",
        "  return Embedding.represent(fiction_embeddings,word)/np.linalg.norm(Embedding.represent(fiction_embeddings,word))\n",
        "\n",
        "\n",
        "def cosine_similarity(vec1,vec2):\n",
        "  return np.dot(vec1, vec2)/(np.linalg.norm(vec1)* np.linalg.norm(vec2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R51BR_eHNfUg"
      },
      "source": [
        "def cosineToCSV(csvName,entitiesList,moralList, startOfDec, model):\n",
        "  from representations.embedding import Embedding\n",
        "  if __name__ == \"__main__\":\n",
        "    fiction_embeddings = Embedding.load(\"/content/drive/My Drive/word2vecProject/histwords/embeddings/{}/{}\".format(model, startOfDec))\n",
        "\n",
        "  with open(csvName, 'wt') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    moralList.insert(0, startOfDec)\n",
        "\n",
        "    # Write the headings to the csv.\n",
        "    writer.writerow(moralList)\n",
        "\n",
        "    moralList.pop(0)\n",
        "  \n",
        "    for entity in entitiesList:\n",
        "      listOfCosines = []\n",
        "      listOfCosines.append(entity)\n",
        "    \n",
        "      for moral in moralList:\n",
        "        sim = cosine_similarity(Embedding.represent(fiction_embeddings,entity)/np.linalg.norm(Embedding.represent(fiction_embeddings,entity)),Embedding.represent(fiction_embeddings,moral)/np.linalg.norm(Embedding.represent(fiction_embeddings,moral)))\n",
        "        if np.isnan(sim):\n",
        "          listOfCosines.append('NA')\n",
        "        else: \n",
        "          listOfCosines.append(sim)\n",
        "\n",
        "      # Writing the cosine scores to the csv.\n",
        "      writer.writerow(listOfCosines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7y48fbrcp6l"
      },
      "source": [
        "Lists of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUq1oP1eB27j"
      },
      "source": [
        "moral_concern_list = ['care', 'cares', 'caring', 'cared', 'concern', 'concerns', 'concerned', 'concerning', 'empathy', 'empathetic', 'empathize', 'sympathy', 'sympathize', 'sympathetic', 'compassion', 'compassionate', 'uncaring', 'apathetic', 'apathy', 'unconcerned', 'unconcerning', 'indifference', 'indifferent', 'unsympathetic', 'unempathetic', 'uncompassionate', 'disregard', 'disregarded', 'disregarding']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVsxux5uP1ii"
      },
      "source": [
        "humans_animals = ['husband', 'wife', 'father', 'mother', 'son', 'daughter', 'brother', 'sister', 'uncle', 'aunt', 'niece', 'nephew', 'grandmother', 'grandfather', 'acquaintance', 'ally', 'associate', 'colleague', 'counterpart', 'fellow', 'neighbor', 'patriot', 'confidant', 'companion', 'partner', 'supporter', 'member', 'follower', 'emigrant', 'foreigner', 'intruder', 'settler', 'stranger', 'visitor', 'vagrant', 'opposition', 'rival', 'opponent', 'adversary', 'competitor', 'invader', 'occupier', 'arab', 'beggar', 'blacks', 'crippled', 'disabled', 'jew', 'mexican', 'unemployed', 'native', 'elderly', 'indian', 'woman', 'chinese', 'pauper', 'animal', 'ape', 'bird', 'elephant', 'chicken', 'cow', 'dog', 'fish', 'pig', 'shark', 'bear', 'snake', 'monkey', 'lion', 'nature', 'forest', 'lake', 'mountain', 'ocean', 'reef', 'river', 'tree', 'sea', 'beach', 'island', 'coast', 'earth', 'planet']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLoRFHxXdJfV"
      },
      "source": [
        "Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUTU8aarC4Gv"
      },
      "source": [
        "list_of_models = ['eng-all_sgns','Genre-Balanced_American_English_(1830s-2000s)']\n",
        "\n",
        "# 'eng-all_sgns', 'Genre-Balanced_American_English_(1830s-2000s)'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI_TR8g3OrZm"
      },
      "source": [
        "Computing the cosine similarity between each entity word and each moral concern word.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMp-ejPCNx0S"
      },
      "source": [
        "for model in list_of_models:  \n",
        "  for i in range(1830,2000,10):\n",
        "    cosineToCSV('entities_and_moral_concern_cosines_{}_{}.csv'.format(model,i), entities_list, moral_concern_list, i, model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}